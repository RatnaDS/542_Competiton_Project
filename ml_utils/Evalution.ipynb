{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evalution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pwY3-qhmFkD"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "class Evaluation:\n",
        "\n",
        "  def __init__(self):\n",
        "        pass\n",
        "  \n",
        "  def aggregate_metrics(y, y_hat):\n",
        "    \n",
        "    \"\"\" Metrics for entire set \n",
        "        \n",
        "        Arguments passed are: \n",
        "                      y     : ground truth  \n",
        "                      y_hat : predicted class\n",
        "        Calculations:\n",
        "                  accuracy  : (tp + tn) / (total samples =(p + n))\n",
        "                  precision : tp / (tp + fp)\n",
        "                  recall    : tp / (tp + fn)\n",
        "                  f1        : 2 tp / (2 tp + fp + fn)\n",
        "    \"\"\"\n",
        "    accuracy  = accuracy_score(y, y_hat)\n",
        "    precision = precision_score(y, y_hat)\n",
        "    recall    = recall_score(y, y_hat)\n",
        "    f1        = f1_score(y, y_hat)\n",
        "    return {\n",
        "        \"Accuracy\"     : accuracy,\n",
        "        \"Precision\"    : precision,\n",
        "        \"Recall Score\" : recall,\n",
        "        \"F1 Score\"     : f1\n",
        "        }\n",
        "\n",
        "  def classwise_metrics(y, y_hat):\n",
        "\n",
        "    \"\"\" Metrics for each class. \n",
        "        Average is set to macro for calculating the score of each label, and find their unweighted mean. \n",
        "        This does not take label imbalance into account.\n",
        "        \n",
        "        Arguments passed are: \n",
        "                      y     : ground truth  \n",
        "                      y_hat : predicted class\n",
        "        Calculations:\n",
        "                  accuracy  : (tp + tn) / (total samples =(p + n))\n",
        "                  precision : tp / (tp + fp)\n",
        "                  recall    : tp / (tp + fn)\n",
        "                  f1        : 2 tp / (2 tp + fp + fn)\n",
        "    \"\"\"\n",
        "    \n",
        "    Class_precision = precision_score(y, y_hat, average='macro')\n",
        "    Class_recall    = recall_score(y, y_hat, average='macro')\n",
        "    Class_f1        = f1_score(y, y_hat, average='macro')\n",
        "    \n",
        "    return {\n",
        "        \"Class-wise Precision\"    : Class_precision,\n",
        "        \"Class-wise Recall Score\" : Class_recall,\n",
        "        \"Class-wise F1 Score\"     : Class_f1\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}